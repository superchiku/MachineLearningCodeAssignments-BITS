{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Text Classification on the data. \n",
    "\n",
    "The tweets have been pulled from Twitter related to coronavirus and manual tagging has been done then.\n",
    "\n",
    "The names and usernames have been given codes to avoid any privacy concerns.\n",
    "\n",
    "You have to only use \"OriginalTweet\" and \"sentiment\" column for the assignment. You can either drop or ignore other columns for this assignment.\n",
    "\n",
    "# 1) Import required libraries - 2 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# for integer encoding using sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime as dt\n",
    "from tkinter import *\n",
    "from tkinter.font import Font\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import scikitplot as skplt\n",
    "from sklearn.preprocessing import scale\n",
    "from pyclustertend import hopkins\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm \n",
    "from math import sqrt \n",
    "from time import gmtime, strftime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "##print(os.listdir(\"../input\"))\n",
    "sns.set()\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import DBSCAN,AgglomerativeClustering, KMeans\n",
    "from apyori import apriori\n",
    "######## Capture the start time to check the runt ime of the whole notebook #########\n",
    "startTime = strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re  #regular expression\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "sw = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "url_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Read dataset and perfom Text processing for the tweets ( Remove Stop words , special characters and convert the text to lowercase ) - 3 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           OriginalTweet           Sentiment\n",
      "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
      "1      advice Talk to your neighbours family to excha...            Positive\n",
      "2      Coronavirus Australia: Woolworths to give elde...            Positive\n",
      "3      My food stock is not the only one which is emp...            Positive\n",
      "4      Me, ready to go at supermarket during the #COV...  Extremely Negative\n",
      "...                                                  ...                 ...\n",
      "41152  Airline pilots offering to stock supermarket s...             Neutral\n",
      "41153  Response to complaint not provided citing COVI...  Extremely Negative\n",
      "41154  You know itÂ’s getting tough when @KameronWild...            Positive\n",
      "41155  Is it wrong that the smell of hand sanitizer i...             Neutral\n",
      "41156  @TartiiCat Well new/used Rift S are going for ...            Negative\n",
      "\n",
      "[41157 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Extract and read text files\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "\n",
    "\n",
    "try:\n",
    "    tweets = pd.read_csv(\"Corona_NLP_train.csv\",encoding=\"mbcs\")\n",
    "    #tweets.columns = ['OriginalTweet', 'Sentiment']\n",
    "    tweets = tweets.drop(['UserName', 'ScreenName','Location','TweetAt'], axis = 1)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Exception occured ',str(e))\n",
    "    \n",
    "    \n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:58: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:58: DeprecationWarning: invalid escape sequence \\d\n",
      "<ipython-input-3-dc4875ae85da>:58: DeprecationWarning: invalid escape sequence \\d\n",
      "  text = re.sub('[^a-zA-Z0-9.\\d\\s]', '', text)\n"
     ]
    }
   ],
   "source": [
    "#Sentence curation functions\n",
    "def stopwords(text):\n",
    "    '''a function for removing the stopword'''\n",
    "    \n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    #text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    # joining the list of words with space separator\n",
    "    text = remove_stopwords(text)\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    #return(text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    '''a function for removing punctuation'''\n",
    "    import string\n",
    "    # replacing the punctuations with no space, \n",
    "    # which in effect deletes the punctuation marks \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # return the text stripped of punctuation marks\n",
    "    return text.translate(translator)\n",
    "\n",
    "def stemming(text):    \n",
    "    \n",
    "    '''a function which stems each word in the given text'''\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)     \n",
    "\n",
    "def remove_html(text):\n",
    "    '''\n",
    "    remove the HTML tags and URLS from the tweets\n",
    "    '''\n",
    "    if text:\n",
    "        # BeautifulSoup on content\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        # Stripping all <code> tags with their content if any\n",
    "        if soup.code:\n",
    "            soup.code.decompose()\n",
    "        # Get all the text out of the html\n",
    "        text =  soup.get_text()\n",
    "        # Returning text stripping out all uris\n",
    "        return re.sub(url_re, \"\", text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def remove_emojis(text):\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        r\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        r\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        r\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        r\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)# no emoji \n",
    "  \n",
    "    return text\n",
    "\n",
    "def remove_specialChars(text):\n",
    "    text = re.sub('[^a-zA-Z0-9.\\d\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           OriginalTweet           Sentiment\n",
      "0                            menyrbie philgahan chrisitv             Neutral\n",
      "1      advice talk neighbours family exchange phone n...            Positive\n",
      "2      coronavirus australia woolworths elderly disab...            Positive\n",
      "3      food stock please dont panic enough food every...            Positive\n",
      "4      ready supermarket covid19 outbreak im paranoid...  Extremely Negative\n",
      "...                                                  ...                 ...\n",
      "41152  airline pilots offering stock supermarket shel...             Neutral\n",
      "41153  response complaint provided citing covid19 rel...  Extremely Negative\n",
      "41154  know its getting tough kameronwilds rationing ...            Positive\n",
      "41155  wrong smell hand sanitizer starting turn coron...             Neutral\n",
      "41156  tartiicat well newused rift going 70000 amazon...            Negative\n",
      "\n",
      "[41157 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## Remove unwanted words ##\n",
    "\n",
    "tweets_duplicate = tweets.copy()\n",
    "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(remove_html)\n",
    "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(remove_emojis)\n",
    "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(remove_punctuation)\n",
    "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(stopwords)\n",
    "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(remove_specialChars)\n",
    "tweets_duplicate = tweets.copy()\n",
    "\n",
    "\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Using the train_test_split function of Sklearn, Split train and test dataset - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 2 ... 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "#Label Encode Y column \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# Encode labels in column 'Sentiment'.\n",
    "tweets['Sentiment']= label_encoder.fit_transform(tweets['Sentiment'])\n",
    "\n",
    "# seperate X,Z and y variables\n",
    "y = tweets['Sentiment']\n",
    "X = tweets['OriginalTweet']\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "#X.shape\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Create pipeline and define parameters for GridSearch ( You might Refer the code below ) - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "\n",
    "text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Perform classification (using GridSearch) - 3 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.3min finished\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:844: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(text_clf, tuned_parameters, cv=5, verbose=1,n_jobs=-1)\n",
    "result = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Print the confusion matrix, accuracy, F1 score on the test dataset - 2 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[ 336    9 1012   21  265]\n",
      " [   3  479  164   32 1320]\n",
      " [ 113   46 1622  127 1067]\n",
      " [  17   40  526  723  984]\n",
      " [  17  206  718  134 2367]]\n",
      "accuracy score =  44.76 %\n",
      "classification report is as below: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.20      0.32      1643\n",
      "           1       0.61      0.24      0.34      1998\n",
      "           2       0.40      0.55      0.46      2975\n",
      "           3       0.70      0.32      0.43      2290\n",
      "           4       0.39      0.69      0.50      3442\n",
      "\n",
      "    accuracy                           0.45     12348\n",
      "   macro avg       0.56      0.40      0.41     12348\n",
      "weighted avg       0.53      0.45      0.43     12348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import scikitplot as skplt\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = result.predict(X_test)\n",
    "#y_pred_prob = grid_clf_acc.predict_proba(X_test)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "#print('Best Hyperparameters: %s' % result.best_params_)\n",
    "print(\"Confusion matrix \\n\",confusion_matrix)\n",
    "print (\"accuracy score = \",round(acc_score*100,2),\"%\")\n",
    "\n",
    "\n",
    "#print classification report \n",
    "print('classification report is as below: \\n', classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at :  Sun, 11 Jul 2021 17:06:45 +0000\n",
      "Run ended at :  Sun, 11 Jul 2021 17:08:11 +0000\n"
     ]
    }
   ],
   "source": [
    "endTime = strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())\n",
    "print (\"Run started at : \",startTime)\n",
    "print (\"Run ended at : \",endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
