{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 6 Mini Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Term - Frequency Inverse Document Frequency \n",
    "\n",
    "1) Remove Stopwords (1 Mark)\n",
    "\n",
    "2) Remove the punctuations. the special characters and convert the text to lower case.        (2 Mark)\n",
    "\n",
    "3) create bigrams and trigrams for the entire dataset and list down 20 most frequent bigram and 20 most frequent trigrams ( 3 Marks )\n",
    "\n",
    "4) You have to implement TF-IDF the Algorithm from scratch.   ( 3 Mark )\n",
    "\n",
    "5) Use the above-implemented algorithm and the values to calculate TF-IDF (using TF IDF formula) on the dataset and list down the top 10 words which have the highest TF-IDF Value. (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "C:\\Users\\jaghos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f731a38e63f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m  \u001b[1;31m#regular expression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# bring model classes directly into package namespace, to save some typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoherencemodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhdpmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHdpModel\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m from gensim.topic_coherence import (segmentation, probability_estimation,\n\u001b[0m\u001b[0;32m     37\u001b[0m                                     \u001b[0mdirect_confirmation_measure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindirect_confirmation_measure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                     aggregation)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m from gensim.topic_coherence.text_analysis import (\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mCorpusAccumulator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWordOccurrenceAccumulator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     WordVectorsAccumulator)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeep_vocab_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcall_on_class_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_any2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m from gensim.models.utils_any2vec import (\n\u001b[0m\u001b[0;32m    184\u001b[0m     \u001b[0m_save_word2vec_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0m_load_word2vec_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     from gensim.models._utils_any2vec import (\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mcompute_ngrams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mcompute_ngrams_bytes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# for integer encoding using sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime as dt\n",
    "from tkinter import *\n",
    "from tkinter.font import Font\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from pyclustertend import hopkins\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm \n",
    "from math import sqrt \n",
    "from time import gmtime, strftime\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "##print(os.listdir(\"../input\"))\n",
    "sns.set()\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import DBSCAN,AgglomerativeClustering, KMeans\n",
    "from apyori import apriori\n",
    "######## Capture the start time to check the runt ime of the whole notebook #########\n",
    "startTime = strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re  #regular expression\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "sw = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "url_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and read text files\n",
    "\n",
    "sentence = pd.DataFrame()\n",
    "file1 = open('TF-IDF_dataset-Copy.txt', 'r',encoding=\"mbcs\")\n",
    "\n",
    "# Using readlines()\n",
    "try:\n",
    "    \n",
    "    \n",
    "    Lines = file1.readlines()\n",
    " \n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    chapter = 'Chapter 1'\n",
    "    for line in Lines:\n",
    "        count += 1\n",
    "        \n",
    "        if len(line.strip()) > 0 and line.strip().find('Chapter') == -1 :\n",
    "            current_df = pd.DataFrame({'sentences': [ line.strip()] , 'chapter':[chapter]})\n",
    "            sentence = sentence.append(current_df, ignore_index=True)\n",
    "        else:\n",
    "            if line.strip().find('Chapter') != -1:\n",
    "                \n",
    "                chapter = line.strip()\n",
    "                #print(chapter)\n",
    "            \n",
    "    file1.close() \n",
    "\n",
    "except Exception as e:\n",
    "    print('Exception occured ',str(e))\n",
    "    file1.close\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence curation functions\n",
    "def stopwords(text):\n",
    "    '''a function for removing the stopword'''\n",
    "    \n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    #text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    # joining the list of words with space separator\n",
    "    text = remove_stopwords(text)\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    #return(text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    '''a function for removing punctuation'''\n",
    "    import string\n",
    "    # replacing the punctuations with no space, \n",
    "    # which in effect deletes the punctuation marks \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # return the text stripped of punctuation marks\n",
    "    return text.translate(translator)\n",
    "\n",
    "def stemming(text):    \n",
    "    \n",
    "    '''a function which stems each word in the given text'''\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)     \n",
    "\n",
    "def remove_html(text):\n",
    "    '''\n",
    "    remove the HTML tags and URLS from the tweets\n",
    "    '''\n",
    "    if text:\n",
    "        # BeautifulSoup on content\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        # Stripping all <code> tags with their content if any\n",
    "        if soup.code:\n",
    "            soup.code.decompose()\n",
    "        # Get all the text out of the html\n",
    "        text =  soup.get_text()\n",
    "        # Returning text stripping out all uris\n",
    "        return re.sub(url_re, \"\", text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def remove_emojis(text):\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        r\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        r\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        r\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        r\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)# no emoji \n",
    "  \n",
    "    return text\n",
    "\n",
    "def remove_specialChars(text):\n",
    "    text = re.sub('[^a-zA-Z0-9.\\d\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove unwanted words ##\n",
    "\n",
    "sentence_duplicate = sentence.copy()\n",
    "sentence['sentences'] = sentence['sentences'].apply(remove_html)\n",
    "sentence['sentences'] = sentence['sentences'].apply(remove_emojis)\n",
    "sentence['sentences'] = sentence['sentences'].apply(remove_punctuation)\n",
    "sentence['sentences'] = sentence['sentences'].apply(stopwords)\n",
    "sentence['sentences'] = sentence['sentences'].apply(remove_specialChars)\n",
    "sentence_duplicate = sentence.copy()\n",
    "#sentence['sentences'] = sentence['sentences'].apply(stemming)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the BIGrams and the TRIGrams ##\n",
    "tokenized_text = []\n",
    "\n",
    "for index , row in sentence.iterrows():\n",
    "    #print(sentence)\n",
    "    sequence = word_tokenize(row[\"sentences\"]) \n",
    "    tokenized_text.extend(sequence) \n",
    "#Words = sentence['sentences'].tolist()\n",
    "#hf = [\"\".join(review) for review in sentence['sentences'].values]\n",
    "#print(hf)\n",
    "#print(tokenized_text)\n",
    "print('20 most frequent bigrams \\n',(pd.Series(nltk.ngrams(tokenized_text, 2)).value_counts())[:20])\n",
    "print('\\n20 most frequent trigrams \\n',(pd.Series(nltk.ngrams(tokenized_text, 3)).value_counts())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement TF- IDF ##\n",
    "\n",
    "# Create the dictionary of words\n",
    "final_dictionary=set()\n",
    "for index , row in sentence.iterrows():\n",
    "    split_sentence = row['sentences'].split(\" \")\n",
    "    set_sentence =  set(split_sentence)\n",
    "    final_dictionary = final_dictionary.union(set_sentence)\n",
    "print('length of word dictionary', len(final_dictionary))\n",
    "\n",
    "\n",
    "wordDict = []#pd.DataFrame()\n",
    "for index , row in tqdm(sentence.iterrows()):\n",
    "    split_sentence = row['sentences'].split(\" \")\n",
    "    wordDictA = dict.fromkeys(final_dictionary, 0)\n",
    "    for word in split_sentence:\n",
    "        wordDictA[word]+=1\n",
    "    #print([wordDictA])\n",
    "    wordDict.append(wordDictA.copy())\n",
    "    #print(wordDict)\n",
    "#print(len(wordDict))\n",
    "\n",
    "\n",
    "#Create the Term Frequency for each sentence\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "wordDictTF = pd.DataFrame()\n",
    "for index , row in tqdm(sentence.iterrows()):\n",
    "    split_sentence = row['sentences'].split(\" \")\n",
    "    wordDictA = dict.fromkeys(final_dictionary, 0)\n",
    "    for word in split_sentence:\n",
    "        wordDictA[word]+=1\n",
    "    #print([wordDictA])\n",
    "    tfSentence = computeTF(wordDictA, split_sentence)\n",
    "    wordDictTF = wordDictTF.append(tfSentence,ignore_index = True)\n",
    "    #print(wordDict)\n",
    "#print(\"TF word dictionary \\n\",wordDictTF)\n",
    "\n",
    "#Create the IDF for the corpus\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word , val in idfDict.items():\n",
    "        wordcount = 0\n",
    "        for itemlist in docList:\n",
    "            if itemlist[word] > 0:\n",
    "                wordcount = wordcount+1\n",
    "        #print(\"word is \",word)\n",
    "        #print(\"val is \",wordcount )\n",
    "        \n",
    "        idfDict[word] = math.log10(N / (float(wordcount) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "idfs = computeIDF(wordDict)\n",
    "#print(idfs)\n",
    "\n",
    "# Calculate TF- IDF\n",
    "FinalTFIDF = pd.DataFrame()\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "for index , row in tqdm(wordDictTF.iterrows()):\n",
    "    tfIDF = computeTFIDF(row.to_dict(), idfs)\n",
    "    FinalTFIDF = FinalTFIDF.append(tfIDF,ignore_index = True)\n",
    "\n",
    "# Print the TF IDF Vector final\n",
    "print(\"Final TD IDF Vector: \\n\",FinalTFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words with highest TF-IDF value\n",
    "\n",
    "print(\"Top 10 words with the highest TFIDF value:\\n\",FinalTFIDF.mean().sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract TF-IDF Vector using library - Extra done to check the accuracy of the self implemented TF-IDF algorithm ##\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "# convert th sentences into a matrix\n",
    "#hf = sentence['sentences'].values #[\" \".join(review) for review in sentence['sentences'].values]\n",
    "hf = [\" \".join(review) for review in sentence['sentences'].values]\n",
    "'''\n",
    "for review in tqdm(sentence['sentences'].values):\n",
    "    hf = hf + \" \" + review\n",
    "'''\n",
    "#print(hf)\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(sentence['sentences'])# becauase TFIDF vectorizer takes texts and not lists\n",
    "#retrieve the terms found in the corpora\n",
    "\n",
    "\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)#index = ['Doc1','Doc2'])\n",
    "\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)\n",
    "\n",
    "\n",
    "print(\"Top 10 words with the highest TFIDF value:\\n\",df_tfidfvect.mean().sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Part of Speech Tagging using the Viterbi Algorithm, \n",
    "\n",
    "\n",
    "6) Label the cleaned Tf-IDF dataset ( obtained after performing step 1 and step 2 )   ( 2 Mark )\n",
    "\n",
    "7) Split the Train and the Test Dataset                      (1 Mark)\n",
    "\n",
    "8) Implement the Viterbi Algorithm ( you can use Library) to get the Part of Speech Tagging.        ( 3 Marks)\n",
    "\n",
    "9) Calculate the Accuracy and F1 score. ( Number of Predicted Correct Tag in the test set / Total number of Data points in the test set)   (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the POS tagging label for cleaned corpus ##\n",
    "tagged_text = []\n",
    "for index , row in sentence_duplicate.iterrows():\n",
    "      \n",
    "    # Word tokenizers is used to find the words \n",
    "    # and punctuation in a string\n",
    "    wordsList = nltk.word_tokenize(row[\"sentences\"])\n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    tagged_text.extend([tagged])\n",
    "\n",
    "print(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the tagged sentences\n",
    "#sent_tag = brown.tagged_sents()\n",
    "mod_sent_tag=[]\n",
    "for s in tagged_text:\n",
    "  s.insert(0,('##','##'))\n",
    "  s.append(('&&','&&'))\n",
    "  mod_sent_tag.append(s)\n",
    "print(mod_sent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_data,test_data =train_test_split(mod_sent_tag,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "print(train_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary whose keys are tags and values contain words which were assigned the correspoding tag\n",
    "# ex:- 'TAG':{word1: count(word1,'TAG')}\n",
    "train_word_tag = {}\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      try:\n",
    "        train_word_tag[t][w]+=1\n",
    "      except:\n",
    "        train_word_tag[t][w]=1\n",
    "    except:\n",
    "      train_word_tag[t]={w:1}\n",
    "print(train_word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the emission probabilities using train_word_tag\n",
    "train_emission_prob={}\n",
    "for k in train_word_tag.keys():\n",
    "  train_emission_prob[k]={}\n",
    "  count = sum(train_word_tag[k].values())\n",
    "  for k2 in train_word_tag[k].keys():\n",
    "    train_emission_prob[k][k2]=train_word_tag[k][k2]/count\n",
    "print(train_emission_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating the bigram of tags to be used for transition probability\n",
    "bigram_tag_data = {}\n",
    "for s in train_data:\n",
    "  bi=list(nltk.bigrams(s))\n",
    "  for b1,b2 in bi:\n",
    "    try:\n",
    "      try:\n",
    "        bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]][b2[1]]=1\n",
    "    except:\n",
    "      bigram_tag_data[b1[1]]={b2[1]:1}\n",
    "print(bigram_tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the probabilities of tag bigrams for transition probability  \n",
    "bigram_tag_prob={}\n",
    "for k in bigram_tag_data.keys():\n",
    "  bigram_tag_prob[k]={}\n",
    "  count=sum(bigram_tag_data[k].values())\n",
    "  for k2 in bigram_tag_data[k].keys():\n",
    "    bigram_tag_prob[k][k2]=bigram_tag_data[k][k2]/count\n",
    "print(bigram_tag_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the possible tags for each word\n",
    "#Note: Here we have used the whole data(Train+Test)\n",
    "#Reason: There may be some words which are not present in train data but are present in test data \n",
    "tags_of_tokens = {}\n",
    "count=0\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n",
    "        \n",
    "for s in test_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n",
    "\n",
    "print(tags_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the test data into test words and test tags\n",
    "test_words=[]\n",
    "test_tags=[]\n",
    "for s in test_data:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (w,t) in s:\n",
    "    temp_word.append(w.lower())\n",
    "    temp_tag.append(t)\n",
    "  test_words.append(temp_word)\n",
    "  test_tags.append(temp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executing the Viterbi Algorithm\n",
    "predicted_tags = []                #intializing the predicted tags\n",
    "for x in range(len(test_words)):   # for each tokenized sentence in the test data\n",
    "  s = test_words[x]\n",
    "  #storing_values is a dictionary which stores the required values\n",
    "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
    "  storing_values = {}              \n",
    "  for q in range(len(s)):\n",
    "    step = s[q]\n",
    "    #for the starting word of the sentence\n",
    "    if q == 1:                \n",
    "      storing_values[q] = {}\n",
    "      tags = tags_of_tokens[step]\n",
    "      for t in tags:\n",
    "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "        try:\n",
    "          storing_values[q][t] = ['##',bigram_tag_prob['##'][t]*train_emission_prob[t][step]]\n",
    "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "        except:\n",
    "          storing_values[q][t] = ['##',0.0001]#*train_emission_prob[t][step]]\n",
    "    \n",
    "    #if the word is not at the start of the sentence\n",
    "    if q>1:\n",
    "      storing_values[q] = {}\n",
    "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "      current_states  = tags_of_tokens[step]               # loading the current states\n",
    "      #calculation of the best previous state for each current state and then storing\n",
    "      #it in storing_values\n",
    "      for t in current_states:                             \n",
    "        temp = []\n",
    "        for pt in previous_states:                         \n",
    "          try:\n",
    "            temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step])\n",
    "          except:\n",
    "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "        max_temp_index = temp.index(max(temp))\n",
    "        best_pt = previous_states[max_temp_index]\n",
    "        storing_values[q][t]=[best_pt,max(temp)]\n",
    "\n",
    "  #Backtracing to extract the best possible tags for the sentence\n",
    "  pred_tags = []\n",
    "  total_steps_num = storing_values.keys()\n",
    "  last_step_num = max(total_steps_num)\n",
    "  for bs in range(len(total_steps_num)):\n",
    "    step_num = last_step_num - bs\n",
    "    if step_num == last_step_num:\n",
    "      pred_tags.append('&&')\n",
    "      pred_tags.append(storing_values[step_num]['&&'][0])\n",
    "    if step_num<last_step_num and step_num>0:\n",
    "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0])\n",
    "  predicted_tags.append(list(reversed(pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the accuracy based on tagging each word in the test data.\n",
    "right = 0 \n",
    "wrong = 0\n",
    "total_data_points = 0\n",
    "for i in range(len(test_tags)):\n",
    "  gt = test_tags[i]\n",
    "  total_data_points = total_data_points + len(gt)\n",
    "  pred = predicted_tags[i]\n",
    "  for h in range(len(gt)):\n",
    "    if gt[h] == pred[h]:\n",
    "      right = right+1\n",
    "    else:\n",
    "      wrong = wrong +1 \n",
    "\n",
    "print('Accuracy on the test data is: ',right/(right+wrong))\n",
    "print('Loss on the test data is: ',wrong/(right+wrong))\n",
    "print('F1 Score is:',right/(total_data_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Topic Modelling        \n",
    "\n",
    "10) Using the LDA algorithm create the Topics (10) for the Corpus             (3 Marks)\n",
    "\n",
    "11) List down the 10 words in each of the Topics Extracted.           (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the word corpus ##\n",
    "tagged_text = []\n",
    "for index , row in sentence_duplicate.iterrows():\n",
    "      \n",
    "    # Word tokenizers is used to find the words \n",
    "    # and punctuation in a string\n",
    "    wordsList = nltk.word_tokenize(row[\"sentences\"])\n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged_text.extend([wordsList])\n",
    "\n",
    "print(tagged_text[:4][3][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dictionary ##\n",
    "\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(tagged_text)\n",
    "# Create Corpus\n",
    "texts = tagged_text\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print number of topics ##\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the top 10 Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "#LDAvis_data_filepath = os.path.join('results')\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open('ldavisfile', 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared,f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open('ldavisfile', 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './/results//ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())\n",
    "print (\"Run started at : \",startTime)\n",
    "print (\"Run ended at : \",endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
